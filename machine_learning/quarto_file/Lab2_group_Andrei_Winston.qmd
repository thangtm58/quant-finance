---
title: "Machine Learning: Mathematical Theory and Applications"
subtitle: "Computer Lab 2, Solutions By Group 10"
author: 
  - Minh Thang Trinh (25585391)
  - Ene Andrei-Teodor (25633619)
date: last-modified
format: 
  html:
    self-contained: true
toc: true
execute:
  error: false
language: 
  title-block-author-single: " "
theme: Default
title-block-banner-color: Primary
editor: visual
---

```{=html}
<style>
.boxed-text {
  border: 2px solid black;
  padding: 10px;
  margin: 10px 0;
}
</style>
```
## 1. Bagging and boosting for bike rental data (regression)

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
suppressMessages(library(dplyr))
suppressMessages(library(splines))
bike_data <- read.csv('/Users/thangtm589/Desktop/UTS/37401 Machine Learning/Computer Lab/Lab 2/bike_rental_hourly.csv')

head(bike_data)
bike_data$log_cnt <- log(bike_data$cnt)
bike_data$hour <- bike_data$hr/23 # transform [0, 23] to [0, 1]. 0 is midnight, 1 is 11 PM

# One hot for weathersit
one_hot_encode_weathersit <- model.matrix(~ as.factor(weathersit) - 1,data = bike_data)
one_hot_encode_weathersit  <- one_hot_encode_weathersit[, -1] # Remove reference category
colnames(one_hot_encode_weathersit) <- c('cloudy', 'light rain', 'heavy rain')
bike_data <- cbind(bike_data, one_hot_encode_weathersit)

# One hot for weekday
one_hot_encode_weekday <- model.matrix(~ as.factor(weekday) - 1,data = bike_data)
one_hot_encode_weekday  <- one_hot_encode_weekday[, -1] # Remove reference category
colnames(one_hot_encode_weekday) <- c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat')
bike_data <- cbind(bike_data, one_hot_encode_weekday)

# One hot for weekday
one_hot_encode_season <- model.matrix(~ as.factor(season) - 1,data = bike_data)
one_hot_encode_season  <- one_hot_encode_season[, -1] # Remove reference category
colnames(one_hot_encode_season) <- c('Spring', 'Summer', 'Fall')
bike_data <- cbind(bike_data, one_hot_encode_season)

# Create lags
bike_data_new <- mutate(bike_data, lag1 = lag(log_cnt, 1), lag2 = lag(log_cnt, 2), 
                        lag3 = lag(log_cnt, 3), lag4 = lag(log_cnt, 4), lag24 = lag(log_cnt, 24))

bike_data_new <- bike_data_new[-c(1:24),] # Lost 24 obs because of lagging

# Create training and test data
bike_all_data_train <- bike_data_new[bike_data_new$dteday >= as.Date("2011-01-01") & bike_data_new$dteday <=  as.Date("2012-05-31"), ]
bike_all_data_test <- bike_data_new[bike_data_new$dteday >= as.Date("2012-06-01") & bike_data_new$dteday <=  as.Date("2012-12-31"), ]
X_train <- cbind(1, bike_all_data_train[, c("lag1", "lag2",  "lag3", "lag4", "lag24")])
spline_basis <- ns(bike_all_data_train$hour, df = 10, intercept = FALSE)
X_train <- cbind(X_train, spline_basis)
colnames(X_train)[1] <- "intercept"
knots <- attr(spline_basis, "knots")
variables_to_keep_in_X <- c("yr", "holiday", "workingday", "temp", "atemp", "hum", "windspeed") 
variables_to_keep_in_X <- c(variables_to_keep_in_X, colnames(one_hot_encode_weathersit), colnames(one_hot_encode_weekday), colnames(one_hot_encode_season))
X_train <- cbind(X_train, bike_all_data_train[, variables_to_keep_in_X])
head(X_train)
# Training data
X_train <- as.matrix(X_train)
y_train <- bike_all_data_train$log_cnt
# Test data
y_test <- bike_all_data_test$log_cnt
X_test <- cbind(1, bike_all_data_test[, c("lag1", "lag2",  "lag3", "lag4", "lag24")])
spline_basis_test <- ns(bike_all_data_test$hour, df=10, knots=knots, intercept = FALSE)
X_test <- cbind(X_test, spline_basis_test)
colnames(X_test)[1] <- "intercept"
X_test <- cbind(X_test, bike_all_data_test[, variables_to_keep_in_X])
X_test <- as.matrix(X_test)
```

::: boxed-text
#### ðŸ’ª Problem 1.1

Use the `randomForest()` function in the `randomForest` package to fit three random forest regressions (bagging algorithms) with the `ntree` argument set to, respectively, 50, 100, and 500 (number of trees to grow). In each case, compute the root mean squared error (RMSE) for the training and test datasets.
:::

```{r}
suppressMessages(library(randomForest))

# Fit model 
rF_ntree_50 <- randomForest(X_train, y_train, ntree = 50)
rF_ntree_100 <- randomForest(X_train, y_train, ntree = 100)
rF_ntree_500 <- randomForest(X_train, y_train, ntree = 500)

# Predict value y hat 
# 50 trees
y_hat_train_ntree_50 <- predict(rF_ntree_50, newdata = X_train)
y_hat_test_ntree_50 <- predict(rF_ntree_50, newdata = X_test)
# 100 trees
y_hat_train_ntree_100 <- predict(rF_ntree_100, newdata = X_train)
y_hat_test_ntree_100 <- predict(rF_ntree_100, newdata = X_test)
# 500 trees
y_hat_train_ntree_500 <- predict(rF_ntree_500, newdata = X_train)
y_hat_test_ntree_500 <- predict(rF_ntree_500, newdata = X_test)

# Calculate RMSE
# 50 trees
RMSE_training_ntree_50 <- sqrt(sum((y_train - y_hat_train_ntree_50)^2)/length(y_train))
RMSE_test_ntree_50 <- sqrt(sum((y_test - y_hat_test_ntree_50)^2)/length(y_test))
# 100 trees
RMSE_training_ntree_100 <- sqrt(sum((y_train - y_hat_train_ntree_100)^2)/length(y_train))
RMSE_test_ntree_100 <- sqrt(sum((y_test - y_hat_test_ntree_100)^2)/length(y_test))
# 500 trees
RMSE_training_ntree_500 <- sqrt(sum((y_train - y_hat_train_ntree_500)^2)/length(y_train))
RMSE_test_ntree_500 <- sqrt(sum((y_test - y_hat_test_ntree_500)^2)/length(y_test))

# Print RMSE
cat(paste0("RMSE Training 50-tree:  ", RMSE_training_ntree_50, "\n",
           "RMSE Test 50-tree:      ", RMSE_test_ntree_50, "\n\n",
           "RMSE Training 100-tree: ", RMSE_training_ntree_100, "\n",
           "RMSE Test 100-tree:     ", RMSE_test_ntree_100, "\n\n",
           "RMSE Training 500-tree: ", RMSE_training_ntree_500, "\n",
           "RMSE Test 500-tree:     ", RMSE_test_ntree_500, "\n"
           ))
```

::: boxed-text
#### ðŸ’ª Problem 1.2

Plot a time series plot of the response in the original scale (i.e. counts and not log-counts) for the last week of the test data (last $24\times 7$ observations). In the same figure, plot a time series plot of the fitted values (in the original scale) from Problem 1.1 when using `ntree=50` and `ntree=100.` Comment on the results.
:::

```{r}
# Design time series data and choose the last week of the test data
row_to_keep <- c((nrow(bike_all_data_test)-167):nrow(bike_all_data_test))
time_series <- data.frame(bike_all_data_test$dteday, 
                          bike_all_data_test$hr, 
                          exp(y_test),                # Convert data to original scale
                          exp(y_hat_test_ntree_50),   # Convert data to original scale
                          exp(y_hat_test_ntree_100),  # Convert data to original scale
                          exp(y_hat_test_ntree_500))  # Convert data to original scale
time_series <- time_series[row_to_keep, ] # Keep last week of data
time_series$datetime <- as.POSIXct(paste(time_series[,1], time_series[,2]), format="%Y-%m-%d %H")

# Change column names
colnames(time_series) <- c("dteday", "hr", "y_test", "y_hat_test_ntree_50", "y_hat_test_ntree_100", "y_hat_test_ntree_500", "datetime")

# Plot time series data
suppressMessages(library(ggplot2))

ggplot(data = time_series, aes(x = datetime)) +
  geom_line(aes(y = y_test, colour = "Original"), lwd=1.2) +
  
  # Add line of predicted value from randomForest with 50 trees
  geom_line(aes(y = y_hat_test_ntree_50, colour = "50-tree randomForest"), lty=1) +
  
  # Add line of predicted value from randomForest with 100 trees
  geom_line(aes(y = y_hat_test_ntree_100, colour = "100-tree randomForest"), lty=2) +
  
  scale_colour_manual("", 
                      breaks = c("Original", "50-tree randomForest", "100-tree randomForest"),
                      values = c("red", "green", "blue")) +
  xlab("Datetime") +
  ylab("Counts") + 
  theme(axis.text.x=element_text(angle=60, hjust=1))
```

::: boxed-text
In overall, both `Random Forest` models almost capture the board trend of the original values although data is not well predicted at some peaks. Furthermore, the random-Forest model with 100 trees seems to be improved compared to the model with 50 trees but not significantly in terms of both RMSEs and capturing trends. Theoretically, more trees generally lead to better accuracy, as the forest averages out the predictions of individual trees, reducing variance. However, in this case it indicates that the number of 50 trees to fit `random Forest` model is rather sufficient and it is not really necessary to increase the tree number.
:::

::: boxed-text
#### ðŸ’ª Problem 1.3

Use the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions (boosting algorithms) with the `nrounds` argument set to, respectively, 10, 25, and 50 (max number of boosting iterations). In each case, compute the root mean squared error (RMSE) for the training and test datasets.
:::

```{r}
suppressMessages(library(xgboost))

# Fit model
boosting_10 <- xgboost(data=X_train, label = y_train, nrounds = 10) # 10 boosting iterations
boosting_25 <- xgboost(data=X_train, label = y_train, nrounds = 25) # 25 boosting iterations
boosting_50 <- xgboost(data=X_train, label = y_train, nrounds = 50) # 50 boosting iterations

# Predict value y hat 
# 10 boosting iterations
y_hat_train_boosting_10 <- predict(boosting_10, newdata = X_train)
y_hat_test_boosting_10 <- predict(boosting_10, newdata = X_test)
# 25 boosting iterations
y_hat_train_boosting_25 <- predict(boosting_25, newdata = X_train)
y_hat_test_boosting_25 <- predict(boosting_25, newdata = X_test)
# 50 boosting iterations
y_hat_train_boosting_50 <- predict(boosting_50, newdata = X_train)
y_hat_test_boosting_50 <- predict(boosting_50, newdata = X_test)

# Calculate RMSE
# 10 boosting iterations
RMSE_training_boosting_10 <- sqrt(sum((y_train - y_hat_train_boosting_10)^2)/length(y_train))
RMSE_test_boosting_10 <- sqrt(sum((y_test - y_hat_test_boosting_10)^2)/length(y_test))
# 25 boosting iterations
RMSE_training_boosting_25 <- sqrt(sum((y_train - y_hat_train_boosting_25)^2)/length(y_train))
RMSE_test_boosting_25 <- sqrt(sum((y_test - y_hat_test_boosting_25)^2)/length(y_test))
# 50 boosting iterations
RMSE_training_boosting_50 <- sqrt(sum((y_train - y_hat_train_boosting_50)^2)/length(y_train))
RMSE_test_boosting_50 <- sqrt(sum((y_test - y_hat_test_boosting_50)^2)/length(y_test))

# Print RMSE
cat(paste0("RMSE Training Boosting 10: ", RMSE_training_boosting_10, "\n",
           "RMSE Test Boosting 10:     ", RMSE_test_boosting_10, "\n\n",
           "RMSE Training Boosting 25: ", RMSE_training_boosting_25, "\n",
           "RMSE Test Boosting 25:     ", RMSE_test_boosting_25, "\n\n",
           "RMSE Training Boosting 50: ", RMSE_training_boosting_50, "\n",
           "RMSE Test Boosting 50:     ", RMSE_test_boosting_50, "\n"
           ))
```

::: boxed-text
#### ðŸ’ª Problem 1.4

Plot the response in the original scale for the last week of the test data (last $24\times 7$ observations) vs the corresponding fitted values from the best bagging model in Problem 1.2 and the best boosting model in Problem 1.3. Comment on the results.
:::

```{r}
# Add best model from boosting to time series data
time_series$y_hat_test_boosting_50 <- exp(y_hat_test_boosting_50[row_to_keep])

# Plot time series data
suppressMessages(library(ggplot2))
ggplot(data = time_series, aes(x = datetime)) +
  geom_line(aes(y = y_test, colour = "Original"), lwd=1.2) +
  
  # Add line of predicted value from randomForest with 50 trees
  geom_line(aes(y = y_hat_test_ntree_500, colour = "500-tree randomForest"), lty=1) +
  
  # Add line of predicted value from randomForest with 100 trees
  geom_line(aes(y = y_hat_test_boosting_50, colour = "50-round Boosting"), lty=2) +
  
  scale_colour_manual("", 
                      breaks = c("Original", "500-tree randomForest", "50-round Boosting"),
                      values = c("red", "purple", "green")) +
  xlab("Datetime") +
  ylab("Counts") + 
  theme(axis.text.x=element_text(angle=60, hjust=1)) 
```

::: boxed-text
First, `50-round Boosting` and `50-tree Random Forest` models were considered as the best models in 1.2 and 1.3 since they achieved better RMSEs compared to their counterparts. In overall, both models did a great job of capturing the trend of the original values. The plot indicates that 50-round boosting performed better at some low-valued peak (e.g. on `Dec 26`), this also suggests that the Boosting model was more prone to overfitting, especially where the original values are relatively small (e.g. the period from `Dec 26` to `Dec 27`). Meanwhile, `Random Forest` model tried to sample and average out the value among trained trees which directly helped it reduce the variance of the ensemble. Therefore, we can see that although the deviation of `Random Forest` was relatively high at some low-valued peak, but fitted values were generally smoother, more robust to noise and performed well at major peaks.
:::

## 2. Bagging and boosting for spam email data (classification)

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
# Load file
load(file = '/Users/thangtm589/Desktop/UTS/37401 Machine Learning/Computer Lab/Lab 2/spam_ham_emails.RData')
Spam_ham_emails[, -1] <- scale(Spam_ham_emails[, -1])
Spam_ham_emails['spam'] <- as.factor(Spam_ham_emails['spam'] == 1) # Changing from 1->TRUE, 0->FALSE
levels(Spam_ham_emails$spam) <- c("not spam", "spam")

set.seed(123)
suppressMessages(library(caret))

# Prepare dataset
train_obs <- createDataPartition(y = Spam_ham_emails$spam, p = .75, list = FALSE)
train <- Spam_ham_emails[train_obs, ]
y_train <- train$spam
X_train <- train[, -1]
test <- Spam_ham_emails[-train_obs, ]
y_test <- test$spam
X_test <- test[, -1]
```

::: boxed-text
#### ðŸ’ª Problem 2.1

Use the `randomForest()` function in the `randomForest` package to fit three random forest classification with the `ntree` argument set to, respectively, 50, 100, and 500. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()` (all plots in the same figure). Comment on the results.
:::

```{r}
# Fit model 
suppressMessages(library(randomForest))
rF_ntree_50 <- randomForest(X_train, y_train, ntree = 50)
rF_ntree_100 <- randomForest(X_train, y_train, ntree = 100)
rF_ntree_500 <- randomForest(X_train, y_train, ntree = 500)

# Predict value y hat 
y_prob_hat_test_ntree_50 <- predict(rF_ntree_50, newdata = X_test, type = "prob") # 50 trees
y_prob_hat_test_ntree_100 <- predict(rF_ntree_100, newdata = X_test, type = "prob") # 100 trees
y_prob_hat_test_ntree_500 <- predict(rF_ntree_500, newdata = X_test, type = "prob") # 500 trees

# Plot pROC
suppressMessages(library(pROC))

## Shape the plot into square
par(pty="s")

## Plot 3 ROC curves
### Plot the first curve randomForest with 50 trees 
roc(test$spam, y_prob_hat_test_ntree_50[,2], plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="red", lwd=3, print.auc=TRUE, print.auc.y=60, auc.polygon=TRUE, print.auc.pattern = "AUC: %.5f%%") 

### Plot the second curve randomForest with 100 trees
plot.roc(test$spam, y_prob_hat_test_ntree_100[,2], percent=TRUE, col="darkgreen", lwd=2, print.auc=TRUE, add=TRUE, lty = 2, print.auc.y=50, print.auc.pattern = "AUC: %.5f%%")

### Plot the third curve randomForest with 500 trees
plot.roc(test$spam, y_prob_hat_test_ntree_500[,2], percent=TRUE, col="purple", lwd=2, print.auc=TRUE, add=TRUE, lty = 3, print.auc.y=40, print.auc.pattern = "AUC: %.5f%%")

### Add legend
legend("bottomright", legend=c("50 Trees", "100 Trees", "500 Trees"), col=c("red", "darkgreen","purple"), lty = c(1,2,3), lwd=4, cex = 1.05)

```

::: boxed-text
Similarly to the finding in 1.2, the `ROC curves` indicates that the performance of three model with the number of trees at 50, 100, and 500 respectively are great. The `AUCs` are all close to 98% which determine that three model have the very high capability of correctly classifying the spam and ham emails. However, there is no significant difference among 3 model in terms of both `AUC` values and ROC curves. This mean `50-tree random Forest` model is somehow optimal and we don't need to increase the number of trees.
:::

::: boxed-text
#### ðŸ’ª Problem 2.2

Predict the test data using the random forest classifier in Problem 2.1 with `ntree=100` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and compute the confusion matrix using the `confusionMatrix()` function from the `caret` package.
:::

```{r}
# Set threshhold and predict y hat 
threshold <- 0.5 # Predict spam if probability > threshold
y_hat_test <- as.factor(y_prob_hat_test_ntree_100[,2] > threshold) # Use spam column
levels(y_hat_test) <- c("not spam", "spam")

# Compute confusion matrix
suppressMessages(library(caret))
confusionMatrix(data = test$spam, y_hat_test, positive = "spam")
```

::: boxed-text
#### ðŸ’ª Problem 2.3

Use the `xgboost()` function in the `xgboost` package to fit three extreme gradient boosting regressions with the `nrounds` argument set to, respectively, 10, 25, and 50. In each case, plot the ROC curve and compute the area under the ROC curve (AUC) for the test data using the package `pROC()` (all plots in the same figure). Moreover, add the ROC plot for the random forest in Problem 2.1 with `ntree=100`. Comment on the results.
:::

```{r}
# Design dataset
X_train_xgb <- as.matrix(X_train)
X_test_xgb <- as.matrix(X_test)
y_train_xgb <- as.integer(y_train) - 1
y_test_xgb <- as.integer(y_test) - 1

# Fit the model by xgboost
suppressMessages(library(xgboost))
xgboost_10 <- xgboost(data=X_train_xgb, label=y_train_xgb, nrounds = 10, objective="binary:logistic") # 10 boosting iterations
xgboost_25 <- xgboost(data=X_train_xgb, label=y_train_xgb, nrounds = 25, objective="binary:logistic") # 25 boosting iterations
xgboost_50 <- xgboost(data=X_train_xgb, label=y_train_xgb, nrounds = 50, objective="binary:logistic") # 50 boosting iterations

# Predict the y hat test value
y_prob_hat_test_xgboost_10 <- predict(xgboost_10, newdata = X_test_xgb) # 10 boosting iterations
y_prob_hat_test_xgboost_25 <- predict(xgboost_25, newdata = X_test_xgb) # 25 boosting iterations
y_prob_hat_test_xgboost_50 <- predict(xgboost_50, newdata = X_test_xgb) # 50 boosting iterations

# Plot pROC
suppressMessages(library(pROC))

## Shape the plot into square
par(pty="s")

## Plot 3 ROC curves
### Plot first curve boosting with 10 iterations 
roc(test$spam, y_prob_hat_test_xgboost_10, plot=TRUE, legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage", ylab="True Postive Percentage", col="red", lwd=2, print.auc=TRUE, print.auc.y=75, auc.polygon=TRUE, print.auc.pattern = "AUC: %.5f%%") 

### Plot second curve boosting with 25 iterations
plot.roc(test$spam, y_prob_hat_test_xgboost_25, percent=TRUE, col="darkgreen", lwd=3, print.auc=TRUE, add=TRUE, lty = 2, print.auc.y=65, print.auc.pattern = "AUC: %.5f%%")

### Plot third curve boosting with 50 iterations
plot.roc(test$spam, y_prob_hat_test_xgboost_50, percent=TRUE, col="purple", lwd=2, print.auc=TRUE, add=TRUE, lty = 3, print.auc.y=55, print.auc.pattern = "AUC: %.5f%%")

### Plot fourth curve random forest with 100 trees
plot.roc(test$spam, y_prob_hat_test_ntree_100[,2], percent=TRUE, col="blue", lwd=2, print.auc=TRUE, add=TRUE, lty = 4, print.auc.y=45, print.auc.pattern = "AUC: %.5f%%")

### Add legend
legend("bottomright", legend=c("Boosting 10 iterations", "Boosting 25 iterations", "Boosting 50 iterations", "Random Forest 100 trees"), col=c("red", "darkgreen","purple", "blue"), lty = c(1,2,3,4), lwd=4, cex = 0.8)
```

::: boxed-text
In overall, all 4 ROC curves approached the top-left corner, this indicates that all models already reach the high accuracy when classifying the spam and ham emails. The difference of AUC values among all models is not really significant. In terms of Boosting, the model having 50 iterations achieved the greatest AUC value (around `98.18%`). However, when we increase the number of iterations to 50, the AUC value did not significantly improve. This suggests that higher iteration can be unnecessary and even might lead to overfitting situation. Besides that, `Random Forest` model also achieved good performance in this classification case. Although it got slightly lower AUC values, this model might be useful to avoid overfitting thanks to averaging and sampling data.
:::

::: boxed-text
#### ðŸ’ª Problem 2.4

Predict the test data using the extreme gradient boosting classifier in Problem 2.3 with `nrounds=25` following the rule: if $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$, and compute the confusion matrix using the `confusionMatrix()` function from the `caret` package.
:::

```{r}
# Set threshold and predict y hat 
threshold <- 0.5 # Predict spam if probability > threshold
y_hat_test <- as.factor(y_prob_hat_test_xgboost_25 > threshold) # Use spam column
levels(y_hat_test) <- c("not spam", "spam")

# Compute confusion matrix
suppressMessages(library(caret))
confusionMatrix(data = test$spam, y_hat_test, positive = "spam")
```

## 3. Learning parametric models by gradient based optimisation

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
```

::: boxed-text
#### ðŸ’ª Problem 3.1

Simulate $n=1000$ independent (conditionally on $x$) $y$ observations from the Poisson regression model $$y|\beta_0,\beta_1 \sim \mathrm{Poisson}(\exp\left(\beta_0 + \beta_1x \right)),$$ with the true parameters $\beta_0= 0.3$ and $\beta_1= 2.5$.
:::

```{r}

# Create function to randomly sample 
gen_x <- function(seed=17092000){
  set.seed(seed)
  return(runif(0, 1, n = 1000))
}

# Generate X
x <- gen_x()

# Calculate lambda basing on X
beta <- c(0.3, 2.5)
X <- cbind(1, x)
lambda <- exp(X%*%beta)

# Simulate y from Poisson distribtuion
y <- rpois(n=1000, lambda)

```

::: boxed-text
#### ðŸ’ª Problem 3.2

In the same figure, plot the scatter $\{x_i, y_i\}_{i=1}^n$ and the conditional expected value $\mathbb{E}(y|x)$ as a function of $x$ given the true parameter values.
:::

```{r}
x_grid<-seq(0, 1, length.out=10000)

# Calculate lambda basing on X
X_grid <- cbind(1, x_grid)
lambda_grid <- exp(X_grid%*%beta) # Also conditional expected value of y given x

# Simulate y from Poisson distribtuion
y_grid <- rpois(n=10000, lambda_grid)

# Plot the conditional expected value of y
plot(x_grid, y_grid, cex = 0.5, col="grey", xlab="x", ylab="y", main="Conditional expected value of y")
lines(x_grid, lambda_grid, col="darkblue", lwd=3)
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")

```

::: boxed-text
#### ðŸ’ª Problem 3.3

Derive (analytically) the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.
:::

::: boxed-text
First, we have the given probability mass function of the Poisson model as follows: $$
p(y|\mu)=\frac{\mu^y\exp(-\mu)}{y!}
$$ Where value $\mu$ is: $$
\mu = e^{(\beta_0 + \beta_1x)}
$$ Therefore, the log-likelihood $\ell(\beta_0, \beta_1)$ for that Poisson regression model is: $$\begin{align*}\ell(\beta_0, \beta_1) &= \log p(y_1,y_2,\dots,y_n|x_1,x_2,\dots,x_n,\beta_0, \beta_1) 
\\[5pt]&= \sum_{i=1}^n  \log \frac{\mu_i^{y_i}\exp(-\mu_i)}{y_i!} 
\\[5pt]&= \sum_{i=1}^n \Bigg( y_i\log\mu_i-\mu_i-\log{y_i!} \Bigg)
\\[5pt]&= \sum_{i=1}^n \Bigg( y_i\log(e^{\beta_0 + \beta_1x_i})-e^{\beta_0 + \beta_1x_i}-\log{(y_i!)} \Bigg)
\\[5pt]&= \sum_{i=1}^n \Bigg( y_i(\beta_0 + \beta_1x_i)-e^{\beta_0 + \beta_1x_i}-\log{(y_i!)} \Bigg)
\end{align*}
$$
:::

::: boxed-text
#### ðŸ’ª Problem 3.4

Derive (analytically) the gradient of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.
:::

::: boxed-text
Basing on the above result, the first partial derivative with respect to $\beta_0$ is: $$
\begin{align*}
\frac{\partial}{\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1) 
&= \frac{\partial}{\partial\beta_0}(y_i(\beta_0 + \beta_1x_i)-e^{\beta_0 + \beta_1x_i}-\log{y_i!})
\\[5pt]&= y_i-e^{\beta_0 + \beta_1x_i}
\end{align*}
$$ The second partial derivative with respect to $\beta_1$ is: $$
\begin{align*}
\frac{\partial}{\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) 
&= \frac{\partial}{\partial\beta_1}(y_i(\beta_0 + \beta_1x_i)-e^{\beta_0 + \beta_1x_i}-\log{y_i!})
\\[5pt]&=y_ix_i -e^{\beta_0 + \beta_1x_i}x_i
\\[5pt]&= (y_i-e^{\beta_0 + \beta_1x_i})x_i
\end{align*}
$$ Hence, the gradient of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model is: $$\begin{align*}
\nabla\ell(\beta_0, \beta_1) 
&=\sum_{i=1}^n \nabla \log p(y_i|x_i,\beta_0, \beta_1)
\\[5pt]&= \sum_{i=1}^n \left(\frac{\partial}{\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1), \frac{\partial}{\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) \right)^\top
\\[5pt]&= \sum_{i=1}^n \Bigg(y_i-e^{\beta_0 + \beta_1x_i}, \left(y_i-e^{\beta_0 + \beta_1x_i}\right)x_i \Bigg)^\top.
\end{align*}
$$
:::

::: boxed-text
#### ðŸ’ª Problem 3.5

Code two functions, one that evaluates $\ell(\beta_0, \beta_1)$, and another that evaluates its gradient, i.e. $\nabla\ell(\beta_0, \beta_1)$. Note that the first function is scalar valued, whereas the second is vector valued ($2\times 1$ in our case).
:::

```{r}
# Create function to calculate log-likelihood
log_likelihood_f <- function(x, y, beta) {
  X <- cbind(1, x)
  mu <- exp(X%*%beta)
  p_y <- (mu^y * exp(-mu)) / factorial(y)
  log_likelihood <- sum(log(p_y))
  return(log_likelihood)
}

# Create function to calculate gradient of log-likelihood
log_likelihood_gradient_f <- function(x, y, beta) {
  X <- cbind(1, x)
  beta0_gradient <- sum(y - exp(X%*%beta))
  beta1_gradient <- sum((y - exp(X%*%beta)) *x)
  return(c(beta0_gradient, beta1_gradient))
}
```

::: boxed-text
#### ðŸ’ª Problem 3.6

Implement gradient descent (Algorithm 5.1 in Lindholm et al. (2022)[^1]) to learn the parameters in the Poisson regression model by minimising the cost function $J(\beta_0, \beta_1)$. Implement three different algorithms, with step sizes $\gamma=0.01, 0.1, 0.25$, and using $(0,0)$ as starting value. Note that Algorithm 5.1 has a stopping condition which states that it stops when the change in the parameter updates from one iteration to the other is small enough. If a wrong implementation is given, or the step size is chosen such that the algorithm does not converge, the code might get stuck in an infinite while loop (the condition to exit the loop is never met). For this reason, we here perform 200 iterations of the algorithm only (regardless of convergence or not). Comment on the results.
:::

[^1]: Lindholm, A., WahlstrÃ¶m, N., Lindsten, F. and SchÃ¶n, T. B (2022). Machine Learning - A First Course for Engineers and Scientists. Cambridge University Press.

```{r}
# Create a function to calculate cost from poisson distribution
cost_function_f <- function(x, y, beta) {
  n <- length(y)
  return(-log_likelihood_f(x, y, beta) / n)
}

# Create to calculate gradient of cost from poisson distribution
cost_function_gradient_f <- function(x, y, beta) {
  n <- length(y)
  return(-log_likelihood_gradient_f(x, y, beta) / n)
}

# Create to implement gradient descent
gradient_descent_f <- function(x, y, beta, learning_rate, iterations) {
  # Assign values
  i <- 0
  result <- c()
  
  # Implement algorithm 5.1
  while (i <= iterations) { 
    # Calculate cost 
    cost <- cost_function_f(x, y, beta)
    
    # Store result
    result <- rbind(result, c(i, cost, beta))
    
    # Calculate the gradient of cost function 
    cost_gradient <- cost_function_gradient_f(x, y, beta)
    
    # Update new beta
    beta_new <-  beta - learning_rate * cost_gradient
    delta <- max(abs(beta_new - beta))
    beta <- beta_new
    
    # Increase iteration
    i <- i+1
  }
  colnames(result) <- c("iteration", "cost", "beta0", "beta1")
  return(as.data.frame(result))
}

# Initialize values 
beta <- c(0,0)

# Implement gradient descent with learning rate 0.01 and 200 iterations
gradient_descent_01 <- gradient_descent_f(x, y, beta, 0.01, 200)

# Implement gradient descent with learning rate 0.1 and 200 iterations
gradient_descent_10 <- gradient_descent_f(x, y, beta, 0.1, 200)

# Implement gradient descent with learning rate 0.25 and 200 iterations
gradient_descent_25 <- gradient_descent_f(x, y, beta, 0.25, 200)

# Plot the result
## Add 3 lines related to 3 gradient descents
plot(gradient_descent_01$iteration, gradient_descent_01$cost, xlab="Number of Iteration", ylab="Cost", main="Gradient Descent Performance by Learning Rates", type="l", lwd=2, col="red", xlim=c(0,200), ylim=c(2,10))
# axis(1, at = seq(0, 200, by = 10))

## Add line learning rate 0.10
lines(gradient_descent_10$iteration, gradient_descent_10$cost, lty=1, lwd=2, col="purple")

## Add line learning rate 0.25
lines(gradient_descent_25$iteration, gradient_descent_25$cost, lty=1, lwd=2, col="green")

## Add grids
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")

## Add legend
legend(x = "topright", lty = c(1, 1, 1), col = c("red", "purple", "green"), legend=c("0.01 learning rate", "0.1 learning rate", "0.25 learning rate"))
```

::: boxed-text
The plot indicates that the models having learning rate at `0.01` and `0.1` can converge. Particularly, the model with 0.1 learning rate converge faster from around the 50th iteration while the one using 0.01 learning rate still has the clearly decreasing tendency until the 200th iteration. On the contrary, the cost of the model 0.25 learning rate suddenly drop and then it fluctuates and moves back and forth. Therefore, this model already overshot, tend to oscillate and doesn't converge to a fixed value. This means 0.25 learning rate is high and not suitable for this gradient descent model.
:::

::: boxed-text
#### ðŸ’ª Problem 3.7

Run stochastic gradient descent (Algorithm 5.3 in Lindholm et al. (2022)) for $E=20$ epochs to learn the parameters in the Poisson regression model. Experiment with three different mini-batch sizes, $n_b=10,50,100$ and use the diminishing learning rate $\gamma^{(t)}=0.5/t^{0.6}$. Which mini-batch size seems to converge the fastest?
:::

```{r}
# Create to implement stochastic gradient descent
stochastic_gradient_descent_f <- function(x, y, beta, epoch, nb) {
  # Assign values
  t <- 1
  n <- length(y)
  
  # Set seed
  set.seed(17092000)
  
  # Value at the 0th iteration
  result <- c(0, 0, 0, beta, NA, cost_function_f(x, y, beta))

  # Implement algorithm 5.3
  for (i in 1:epoch) {
    # Randomly shuffle the training data x and y
    shuffle_x <- sample(x)
    shuffle_y <- sample(y)
    for (j in 1:(n/nb)) {
      learning_rate <- 0.5 / (t^0.6)
      x_train <- shuffle_x[((j-1) * nb + 1):(j*nb)]
      y_train <- shuffle_y[((j-1) * nb + 1):(j*nb)]
      d <- cost_function_gradient_f(x_train, y_train, beta)
      
      # Compute new beta
      beta_new <- beta - learning_rate * d

      # Store result
      beta <- beta_new
      cost <- cost_function_f(x, y, beta)
      result <- rbind(result, c(t, i, j, beta, learning_rate, cost))
      
      # Increase t 
      t <- t+1
    }
  }
  
  colnames(result) <- c("iteration", "epoch", "j", "beta0", "beta1", "learning_rate", "cost")
  return(as.data.frame(result))
}

# Initialize values
beta <- c(0, 0)
epoch <- 20

# Implement stochastic gradient descent with learning rate and mini-batch size 10
stochastic_10 <- stochastic_gradient_descent_f(x, y, beta, epoch, 10)

# Implement stochastic gradient descent with learning rate and mini-batch size 50
stochastic_50 <- stochastic_gradient_descent_f(x, y, beta, epoch, 50)

# Implement stochastic gradient descent with learning rate and 100 mini-batch size 100
stochastic_100 <- stochastic_gradient_descent_f(x, y, beta, epoch, 100)

# Plot the result
## Add 3 lines related to 3 gradient descents
plot(stochastic_10$iteration, stochastic_10$cost, xlab="Number of Iteration", ylab="Cost", main="Stochastic Gradient Descent Performance by Mini-batch Sizes", type="l", lwd=2, col="red", xlim=c(0,200), ylim=c(3,10))

## Add line mini-batch size 50
lines(stochastic_50$iteration, stochastic_50$cost, lty=1, lwd=2, col="purple")

## Add line mini-batch size 100
lines(stochastic_100$iteration, stochastic_100$cost, lty=1, lwd=2, col="green")

## Add grids
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")

## Add legend
legend(x = "topright", lty = c(1, 1, 1), col = c("red", "purple", "green"), legend=c("Mini-batch size 10", "Mini-batch size 50", "Mini-batch size 100"))

```

::: boxed-text
First, I chose a small interval of `ylim=c(3,10)` so that we can see the fluctuation more clearly. The figure indicates that green line tend to be stable faster than the other lines while the red line still slightly fluctutates until the 200th iteration. This means that the model having 100 observations at each mini-batch can converge fastest compared to 2 other models. This is because larger mini-batch size can provide a more accurate approximation gradient estimate, it has more data each round to average out and then reduce the variance compared to smaller mini-batch size. Therefore, it leads to more stable and accurate update each round. Meanwhile, at mini-batch size 10, the gradient values can be noisier which result in less stable update.

Now we can check the below trace plot of $\beta_0$ and $\beta_1$ to see the change overtime.
:::

```{r}
# Plot the result
## Add 3 lines related to 3 gradient descents
plot(stochastic_10$iteration, stochastic_10$beta0, xlab="Number of Iteration", ylab="Beta 0", main="Beta 0 by Iteration", type="l", lwd=2, col="red", xlim=c(0,200), ylim=c(-1,3.5))

## Add line mini-batch size 50
lines(stochastic_50$iteration, stochastic_50$beta0, lty=1, lwd=2, col="purple")

## Add line mini-batch size 100
lines(stochastic_100$iteration, stochastic_100$beta0, lty=1, lwd=2, col="green")

## Add grids
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")

## Add legend
legend(x = "bottomright", lty = c(1, 1, 1), col = c("red", "purple", "green"), legend=c("Mini-batch size 10", "Mini-batch size 50", "Mini-batch size 100"))
```

```{r}
# Plot the result
## Add 3 lines related to 3 gradient descents
plot(stochastic_10$iteration, stochastic_10$beta1, xlab="Number of Iteration", ylab="Beta 1", main="Beta 1 by Iteration", type="l", lwd=2, col="red", xlim=c(0,200), ylim=c(-4,1))

## Add line mini-batch size 50
lines(stochastic_50$iteration, stochastic_50$beta1, lty=1, lwd=2, col="purple")

## Add line mini-batch size 100
lines(stochastic_100$iteration, stochastic_100$beta1, lty=1, lwd=2, col="green")

## Add grids
grid(nx = NULL, ny = NULL, col = "lightgray", lty = "dotted")

## Add legend
legend(x = "bottomright", lty = c(1, 1, 1), col = c("red", "purple", "green"), legend=c("Mini-batch size 10", "Mini-batch size 50", "Mini-batch size 100"))
```

::: callout-note
From the above trace plots, we can see that $\beta_0$ and $\beta_1$ of the model having mini-batch size 100 reach the stability faster than others.
:::

## 4. Learning parametric models by second order optimisation (3 marks)

::: boxed-text
#### ðŸ’ª Problem 4.1

Derive (analytically) the Hessian of the log-likelihood $\ell(\beta_0, \beta_1)$ for the Poisson regression model.
:::

::: boxed-text
As above result, we have:

$$\begin{align*} \log p(y_i|x_i,\beta_0, \beta_1) &= y_i(\beta_0 + \beta_1x_i)-e^{\beta_0 + \beta_1x_i}-\log{(y_i!)}. \end{align*} $$

The first order derivatives with respect to $\beta_0$ and $\beta_1$ are, respectively:

$$ \begin{align*} \frac{\partial}{\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1)  &= y_i-e^{\beta_0 + \beta_1x_i} \end{align*} $$

$$ \begin{align*} \frac{\partial}{\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1)  &=  (y_i-e^{\beta_0 + \beta_1x_i})x_i \end{align*} $$

For the Hessian Matrix we will need second order derivatives as follows:

$$\nabla\nabla^\top \log p(y_i|x_i,\beta_0, \beta_1) = \begin{bmatrix} \frac{\partial^2}{\partial\beta_0^2}\log p(y_i|x_i,\beta_0, \beta_1) & \frac{\partial^2}{\partial\beta_0\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) \\ \frac{\partial^2}{\partial\beta_1\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1) & \frac{\partial^2}{\partial\beta_1^2}\log p(y_i|x_i,\beta_0, \beta_1) \end{bmatrix}. $$

We get:

$$ \begin{align*} \frac{\partial^2}{\partial\beta_0^2}\log p(y_i|x_i,\beta_0, \beta_1) &= -e^{\beta_0 + \beta_1x_i} \end{align*} $$

$$ \begin{align*} \frac{\partial^2}{\partial\beta_0\partial\beta_1}\log p(y_i|x_i,\beta_0, \beta_1) &= \frac{\partial^2}{\partial\beta_1\partial\beta_0}\log p(y_i|x_i,\beta_0, \beta_1) = -e^{\beta_0 + \beta_1x_i}x_i \end{align*} $$

$$ \begin{align*}\frac{\partial^2}{\partial\beta_1^2}\log p(y_i|x_i,\beta_0, \beta_1)= -e^{\beta_0 + \beta_1 x_i} x_i^2\end{align*} $$

Therefore, the Hessian Matrix is equivalent to:

$$\nabla\nabla^\top \log p(y_i|x_i,\beta_0, \beta_1) = \begin{bmatrix} -e^{\beta_0 + \beta_1x_i} & -e^{\beta_0 + \beta_1x_i}x_i \\ -e^{\beta_0 + \beta_1x_i}x_i & -e^{\beta_0 + \beta_1x_i}x_i^2 \end{bmatrix}. $$
:::

::: boxed-text
#### ðŸ’ª Problem 4.2

Fill in the `NA` values in the function `Hess_log_dens_single_obs()` above.
:::

```{r}
# Note: Fill in the NAs!
Hess_log_dens_single_obs <- function(beta, y_i, X_i) {
  phi11 <- -(beta_0 + beta_1 * x_i)
  phi12 <- -(beta_0 + beta_1 * x_i) * x_i
  phi21 <- phi12
  phi22 <-  -(beta_0 + beta_1 * x_i) * x_i^2
  return(matrix(c(phi11, phi21, phi12, phi22), nrow = 2, ncol = 2))
}

Hess_log_like <- function(beta, y, X) {
  n <- length(y)
  sum_Hess_log_like <- matrix(rep(0, 4), nrow = 2, ncol = 2)
  for(i in 1:n) {
    sum_Hess_log_like <- sum_Hess_log_like + Hess_log_dens_single_obs(beta, y[i], X[i, ])
  }
  return(sum_Hess_log_like)
}
```

::: boxed-text
#### ðŸ’ª Problem 4.3

Implement a trust region Newton's method (Algorithm 5.2 in Lindholm et al. (2022)) to learn the parameters in the Poisson regression model. Run the algorithm for 200 iterations using starting values $(0,0)$ and trust region radius $D=1$. Compare the convergence of this method to that of the gradient descent method in Problem 3.6.
:::

```{r}
# Log-likelihood function for Poisson regression
log_likelihood_f <- function(x, y, beta) {
  X <- cbind(1, x)
  mu <- exp(X %*% beta)
  log_likelihood <- sum(y * log(mu) - mu - log(factorial(y)))  # Including the log factorial term
  return(log_likelihood)
}

# Gradient of log-likelihood function
log_likelihood_gradient_f <- function(x, y, beta) {
  X <- cbind(1, x)
  mu <- exp(X %*% beta)
  grad_beta0 <- sum(y - mu)
  grad_beta1 <- sum((y - mu) * x)
  return(c(grad_beta0, grad_beta1))
}

# Cost function for Poisson regression
cost_function_f <- function(x, y, beta) {
  n <- length(y)
  return(-log_likelihood_f(x, y, beta) / n)
}

# Gradient of the cost function
cost_function_gradient_f <- function(x, y, beta) {
  n <- length(y)
  return(-log_likelihood_gradient_f(x, y, beta) / n)
}

# Hessian for one observation
Hess_log_dens_single_obs <- function(beta, y_i, X_i) {
  beta_0 <- beta[1]
  beta_1 <- beta[2]
  
  x_i <- X_i[2]  
  
  # Define mu_i
  mu_i <- exp(beta_0 + beta_1 * x_i)
  
  phi11 <- -mu_i
  phi12 <- -mu_i * x_i
  phi21 <- phi12
  phi22 <- -mu_i * x_i^2
  
  return(matrix(c(phi11, phi21, phi12, phi22), nrow = 2, ncol = 2))
}

# Hessian for the entire log-likelihood
Hess_log_like <- function(beta, y, X) {
  n <- length(y)
  sum_Hess_log_like <- matrix(rep(0, 4), nrow = 2, ncol = 2)
  
  for(i in 1:n) {
    sum_Hess_log_like <- sum_Hess_log_like + Hess_log_dens_single_obs(beta, y[i], X[i, ])
  }
  
  return(sum_Hess_log_like)
}

# Trust Region Newton's Method (TRN)
TRN <- function(x, y, max_iter = 200, D = 1) {
  beta <- c(0, 0)  
  n <- length(y)
  cost_storage <- numeric(max_iter)  # renamed J_storage to cost_storage for consistency
  beta_storage <- matrix(0, nrow = max_iter, ncol = 2) 
  
  for (t in 1:max_iter) {
    grad_J <- cost_function_gradient_f(x, y, beta)  
    Hess_J <- -Hess_log_like(beta, y, cbind(1, x)) / n 
    
    # Newton's Step: s = - H^-1 * gradient
    newton_s <- solve(Hess_J) %*% grad_J
    
    # Calculate the norm of the Newton step
    norm_newton_s <- sqrt(sum(newton_s^2))
    
    # Apply the trust region scaling if necessary
    scaling_factor <- D / max(norm_newton_s, D)
    s <- scaling_factor * newton_s  # Scale the step
    
    # Update the beta.
    beta_new <- beta - s
    
    # Store the cost and the parameters.
    cost_storage[t] <- cost_function_f(x, y, beta)
    beta_storage[t, ] <- beta
    
    # Update the parameters.
    beta <- beta_new
  }
  
  return(list(beta = beta, cost_storage = cost_storage, beta_storage = beta_storage))
}

# Sample 
set.seed(17092000)
x <- runif(1000, 0, 1)  
beta_true <- c(0.3, 2.5) 
mu <- exp(beta_true[1] + beta_true[2] * x) 
y <- rpois(1000, mu) 

# Run the Trust Region Newton method (TRN)
TRN_Result <- TRN(x, y, max_iter = 200, D = 1)

# Run Gradient Descent
GRD_Result <- gradient_descent_f(x, y, beta = c(0, 0), learning_rate = 0.1, iterations = 200)

# Plotting and visual comparison
plot(1:length(TRN_Result$cost_storage), TRN_Result$cost_storage, type = "l", col = "purple",
     ylab = "Cost", xlab = "Iterations", main = "Trust Region Newton's vs Gradient Descent",
     ylim = c(min(c(TRN_Result$cost_storage, GRD_Result$cost)),
              max(c(TRN_Result$cost_storage, GRD_Result$cost))))
lines(1:length(GRD_Result$cost), GRD_Result$cost, col = "red")
legend("topright", legend = c("Trust Region Newton", "Gradient Descent"), col = c("purple", "red"), lty = 1)


```

::: boxed-text
The "Trust Region Newton's" Method converges really fast around the first \~ 20 iterations, which is a common behaviour in second-order methods. There is an interesting phenomena we can observe: early in the process, there are some spikes (oscillations) before it calms down. We think that due to the large trust regions, the algorithm takes initially aggressive steps and it quickly adjusts afterwards.

In what concerns the "Gradient Descent", it visibly converges slower. It is true that it takes more iterations to touch the same minimal cost, which is common behaviour for first order methods. It is somehow less computationally efficient. All in all, both methods converge around the same cost value, at \~ 200 repetitions. We think it's up to the researcher / ultimate user to decide which method is more suitable. TRN Method provides faster speed, but, it is rather unstable in early stage. Gradient Descent does the same work, slower, and with less intellectual / computational effort, so, for simple scenarios, it might actually better. For industries where speed matter, obviously, TRN is the better alternative.
:::

## 5. Learning parametric models using the optim function

```{r}
rm(list=ls()) # Remove variables 
cat("\014") # Clean workspace
```

::: boxed-text
#### ðŸ’ª Problem 5.1

Code a cost function $J(\beta)$ that uses the vectorised functions above (`'log_dens_Poisson()'` and `'log_like_Poisson()'`).
:::

```{r}
log_dens_Poisson <- function(beta, y, X) {
  return(dpois(y, lambda = exp(X %*% beta), log = TRUE))
}

# Log-likelihood function (already given)
log_like_Poisson <- function(beta, y, X) {
  return(sum(log_dens_Poisson(beta, y, X)))
}

# Cost function J of (beta) = - log-likelihood / n (we are minimizing the negative log-likelihood)
cost_function_Poisson <- function(beta, y, X) {
  n <- length(y)
  return(-log_like_Poisson(beta, y, X) / n)
}
```

::: boxed-text
#### ðŸ’ª Problem 5.2

Use the `'optim()'` function to minimise the objective function $J(\beta)$. Can you retrieve the true parameter values?
:::

```{r}
# Log-density function for Poisson regression
log_dens_Poisson <- function(beta, y, X) {
  return(dpois(y, lambda = exp(X %*% beta), log = TRUE))
}

# Log-likelihood function
log_like_Poisson <- function(beta, y, X) {
  return(sum(log_dens_Poisson(beta, y, X)))
}

# Cost function J of (beta) = - log-likelihood / n
cost_function_Poisson <- function(beta, y, X) {
  n <- length(y)
  return(-log_like_Poisson(beta, y, X) / n)  # Divide by n for average log-likelihood
}

# Sample
set.seed(17092000)
n <- 1000
x <- runif(n, 0, 1)  
X <- cbind(1, x)     
beta_true <- c(0.3, 2.5)  
y <- rpois(n, lambda = exp(X %*% beta_true))  

# Initial guess for beta
beta_start <- c(0, 0)  

# Use optim() to minimize the cost function
result <- optim(par = beta_start, fn = cost_function_Poisson, method = "L-BFGS-B", y = y, X = X)

# Display the estimated beta values and the true beta values
estimated_beta <- result$par
true_beta <- beta_true
convergence_evaluation <- result$convergence

cat("Estimated beta values:", estimated_beta, "\n")
cat("True beta values:", true_beta, "\n")
cat("Convergence evaluation (0 = success):", convergence_evaluation, "\n")
```

::: boxed-text
There is no need to explain if we retrieved the true parameter values and why we didn't get them exactly. We used this line of code: `convergence_evaluation <- result$convergence` that automatically assesses the quality of the optimization, with `0` meaning `Successful`. Easily observable, we obtained `0`, so, we can say that the method is successful, although we did not get the exact true values. For a more advanced explanation, "L-BFGS-B" method is also known as "Limited Memory BFGS", which is in the family of Quasi-Newton Methods. This is commonly used when the Hessian Matrix is problematic, which, in this context, means not invertible which is equivalent to all eigenvalues being equal to 0. This method is an approximation method for large-scale datasets. It actually uses an estimate of the Hessian matrix, this being the reason why we do not retrieve the exact values of the true parameters. **To provide an exact answer to the very specific question if we actually did or did not retrieve the true parameter values, the answer is: NO!** The reason we do not retain the exact true parameter value is due to the random phenomena. In theory, true parameter values can be retrieved if the number of data points reaches infinity. In practice, this is...unpractical! Our opinion is that we retrieved for this problem (set-up) values that are arbitrarily close to the true parameter values and, by increasing the sample size, we'd be getting closer and closer.
:::

```{r}
file_path <- "/Users/thangtm589/Desktop/UTS/37401 Machine Learning/Computer Lab/Lab 2/eBay_coins.RData"
load(file_path)
str(eBay_coins)  
head(eBay_coins)
```

::: boxed-text
#### ðŸ’ª Problem 5.3

Use the `'optim()'` function to learn the parameters in the Poisson regression model for the eBay dataset. Note that you have to read in the dataset yourself this time.
:::

```{r}
# Log-density function for Poisson regression
log_dens_Poisson <- function(beta, y, X) {
  lambda <- exp(X %*% beta)  # Linear predictor and exponential link
  return(dpois(y, lambda = lambda, log = TRUE))
}

# Log-likelihood function, summing the log-densities for all observations
log_like_Poisson <- function(beta, y, X) {
  return(sum(log_dens_Poisson(beta, y, X)))
}

# Cost function J(beta) = -log-likelihood / n
cost_function_Poisson <- function(beta, y, X) {
  n <- length(y)
  return(-log_like_Poisson(beta, y, X) / n)  # Divide by n to get the average negative log-likelihood
}

# Load the dataset
# Replace this with the correct path if needed
load("/Users/thangtm589/Desktop/UTS/37401 Machine Learning/Computer Lab/Lab 2/eBay_coins.RData")

# Matrix creation and defining the dependent variable
X <- as.matrix(eBay_coins[, c("Const", "PowerSeller", "VerifyID", "Sealed", 
                              "Minblem", "MajBlem", "LargNeg", "LogBook", "MinBidShare")])
y <- eBay_coins$nBids

# Initial parameter values (all set to 0)
beta_start <- rep(0, ncol(X))  

# Use the optim() function with method="L-BFGS-B" to learn the parameters
result <- optim(par = beta_start, fn = cost_function_Poisson, method = "L-BFGS-B", y = y, X = X)

# Extract estimated beta values and convergence assessment
estimated_beta <- result$par  
convergence_assessment <- result$convergence  

# Print the estimated beta values and the convergence assessment
cat("Estimated beta values:", estimated_beta, "\n")
cat("Convergence assessment (0 = success):", convergence_assessment, "\n")

```

::: boxed-text
#### ðŸ’ª Problem 5.4

Suppose there is a new auction with features:

-   `'PowerSeller'=1`,

-   `'VerifyID'=1`,

-   `'Sealed'=1`,

-   `'Minblem'=0`,

-   `'MajBlem'=0`,

-   `'LargNeg'=0`,

-   `'LogBook'=1`,

-   `'MinBidShare'=0.5`.

Provide a point estimate of the expected number of bidders for this auction.
:::

```{r}
# We define a new feature vector.
X_new <- c(1,  # Intercept (Const)
           1,  # PowerSeller
           1,  # VerifyID
           1,  # Sealed
           0,  # Minblem
           0,  # MajBlem
           0,  # LargNeg
           1,  # LogBook
           0.5) # MinBidShare

# Use the estimated beta coefficients from the optim() result
estimated_beta <- result$par

# Calculate the expectation -> estimated expected number of bidders for this action.
lambda_hat <- exp(sum(X_new * estimated_beta))

# Print the point estimate for the expected number of bidders
cat("Expected number of bidders for the new auction:", lambda_hat, "\n")

```
